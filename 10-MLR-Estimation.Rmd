# Parameter Estimation in MLR


<!--- For HTML Only --->
`r if (!knitr:::is_latex_output()) '
$\\newcommand{\\E}{\\mathrm{E}}$
$\\newcommand{\\Var}{\\mathrm{Var}}$
$\\newcommand{\\bmx}{\\bm x}$
$\\newcommand{\\bmX}{\\bm X}$
$\\newcommand{\\bmy}{\\bm y}$
$\\newcommand{\\bmbeta}{\\bm{\\beta}}$
$\\newcommand{\\XtX}{\\bmX^\\mT\\bmX}$
$\\newcommand{\\mT}{\\mathsf{T}}$
$\\newcommand{\\XtXinv}{(\\bmX^\\mT\\bmX)^{-1}}$
'`


## Multiple Linear Regression


$$Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_kx_{ik} +  \epsilon_i$$

* $E[\epsilon_i] = 0$
* $Var(\epsilon_i) = \sigma^2$
* $\epsilon_i$ are uncorrelated




## Example: Denver Broncos

```{r eval=FALSE, include=FALSE}
nfl_den <- read_csv(paste0(data_dir, "nfl2018_denver_offense.csv"))
nfl_den <- nfl_den %>%
    filter(play_type %in% c("pass", "run")) %>%
    filter(!interception, !fumble_lost, !penalty)
```

```{r eval=FALSE, include=FALSE}
summary(lm(yards_gained ~ yardline_100 + ydstogo + play_type, data=nfl_den))
```

* $Y_i =$ Yards gained on play
* $x_{i1} =$ Yards to Endzone
* $x_{i2} =$ Yards to Go 
* $x_{i3} =$ Play Type (0=pass, 1= run)

The MLR model fit to this data is:

$$\hat y_i = 2.84 + 0.036x_{i1} + 0.14x_{i2} - 0.95x_{i3}$$



## Estimation of $\beta$ in MLR

How do we estimate the $\beta_j$'s in the MLR model?  

* Like SLR, minimize the sum of squared residuals $\sum_{i=1}^n e_i^2$
* Gives the *hyperplane* (the $p$-dimensional analogue of a plane) that best fits the data

\begin{align*}
S(\beta_0, \beta_1, \dots, \beta_k) & = \sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1x_{i1} + \dots + \beta_kx_{ik})\right)^2
\end{align*}

To find $\hat\beta_0, \hat\beta_1, \dots, \hat\beta_k$ differentiate $S(\beta_0, \beta_1, \dots, \beta_k)$ with respect to all $(k+1)$ $\beta_j$'s and solve a system of $p=k+1$ equations....  

OR use matrix algebra!

<!-- This gets complicated quickly--see equations (3.9) to (3.11) in textbook. Solution? Use matrix algebra. -->
## Matrix form of the MLR model {#mlrmx}


### Covariance Matrix

The variance of a random variable is $\Var(X) = \E[(X - \E[X])^2]$

The variance of a vector of random variables $x$ is the matrix analogue:

\small 
\begin{align*}
\Var(\bmx) &= \E[(\bmx - \E[\bmx])(\bmx - \E[\bmx])^\mT]\\
&= \begin{bmatrix} \Var(x_1) & \mathrm{Cov}(x_1, x_2) & \cdots & \cdots & \mathrm{Cov}(x_1, x_n)\\ \mathrm{Cov}(x_2, x_1) & \Var(x_2) & &  & \vdots \\
\vdots & & \ddots & &  \vdots\\
\vdots & & &  \ddots & \mathrm{Cov}(x_{n-1}, x_n) \\
\mathrm{Cov}(x_n, x_1) & \cdots & \cdots & \mathrm{Cov}(x_n, x_{n-1}) & \Var(x_n) \end{bmatrix}
\end{align*}



If $\mathbf{A}$ is a $q \times n$ matrix and $\bmy$ is an $n$-vector,  $\Var\left[\mathbf{A}\bmy\right] = \mathbf{A}\Var\left[\bmy\right]\mathbf{A}^\mT$ (a $q \times q$ matrix)


### Matrix form of the MLR model

Write the $x_{ij}$'s as a $(n \times p)$ matrix:\footnote{$p=k+1$} $\mathbf{X} = \begin{bmatrix} 1 & x_{11} & x_{12} & \cdots & x_{1k} \\ 1 & x_{21} & & & \vdots \\ \vdots & \vdots & & & \vdots \\ 1& x_{n1} & \cdots & \cdots & x_{nk} \end{bmatrix}$

Write the $\beta's$ as a $(p \times 1)$ vector: $\bmbeta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix}$.


\begin{align*}
\bmX\bmbeta &= \begin{bmatrix} 1 & x_{11} & x_{12} & \cdots & x_{1k} \\ 1 & x_{21} & & & \vdots \\ \vdots & \vdots & & & \vdots \\ 1& x_{n1} & \cdots & \cdots & x_{nk} \end{bmatrix}\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix}\\
&= \begin{bmatrix} \beta_0 + x_{11}\beta_1 + \dots + x_{1k}\beta_k \\\beta_0 + x_{21}\beta_1 + \dots + x_{2k}\beta_k \\ \vdots \\ \beta_0 + x_{n1}\beta_1 + \dots + x_{nk}\beta_k \end{bmatrix}
\end{align*}


Write the $y_i$'s as an $(n \times 1)$ vector: $\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$.


Write the $\epsilon_i$'s as an $(n \times 1)$ vector: $\boldsymbol{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}$.




\begin{align*}
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} &= \begin{bmatrix} 1 & x_{11} & x_{12} & \cdots & x_{1k} \\ 1 & x_{21} & & & \vdots \\ \vdots & \vdots & & & \vdots \\ 1& x_{n1} & \cdots & \cdots & x_{nk} \end{bmatrix}\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix} + \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}\\
&= \begin{bmatrix} \beta_0 + x_{11}\beta_1 + \dots + x_{1k}\beta_k + \epsilon_1 \\\beta_0 + x_{21}\beta_1 + \dots + x_{2k}\beta_k + \epsilon_2\\ \vdots \\ \beta_0 + x_{n1}\beta_1 + \dots + x_{nk}\beta_k + \epsilon_n\end{bmatrix}
\end{align*}

<!-- Notation:  -->

<!-- * Vectors are denoted by lower-case letters that are bold when typed ($\mathbf{v}$) or underlined when handwritten.   -->
<!-- * Matrices are denoted by upper-case letters that are bold when typed ($\mathbf{X}$) or underline when handwritten.  -->
<!-- * $y_i$ is the $i$th element of y -->
<!-- * $x_{ij}$ is the element in the $i$th row and $j$th column of $\mathbf{X}$. -->
<!-- * $\mathbf{x}^\mT$ transposes the vector $\mathbf{x}$ -->
<!-- * $\bmI$ is an identity matrix (1's on diagonal, zeros everywhere else) -->


The MLR model in (compact) matrix form is: 

\begin{align*}
\mathbf{y} &= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\\
\E[\bmepsilon] &= \mathbf{0} \\
\Var[\bmepsilon] &= \sigma^2\bmI \\
\bmX & \text{ is `full-rank'}
\end{align*}

The assumption that $\Var[\bmepsilon] = \sigma^2\bmI$ implies constant variance ($\Var(\epsilon_i) = \sigma^2$) and no correlation between the $\epsilon_i$'s ($Cov(\epsilon_i, \epsilon_j) = 0$).



### Minimizing Sum of Squared Residuals


The least-squares criterion then becomes:
\begin{align*}
S(\hat\bmbeta) = \sum_{i=1}^n e_i^2 &=\mathbf{e}^\mT\mathbf{e} \qquad\qquad  (\mathbf{e} = \bmy - \bmX\hat\bmbeta)\\
&=\left(\bmy - \bmX\hat\bmbeta\right)^\mT\left(\bmy - \bmX\hat\bmbeta\right)\\
&=\bmy^T\bmy - (\bmX\hat\bmbeta)^\mT\bmy - \bmy^\mT\bmX\hat\bmbeta + (\bmX\hat\bmbeta)^\mT\bmX\hat\bmbeta\\
& =\bmy^T\bmy  - 2\hat\bmbeta^\mT\bmX^\mT\bmy + \hat\bmbeta^\mT\bmX^\mT\bmX\hat\bmbeta
\end{align*}

We minimize this by differentiating: $\frac{S(\boldsymbol\beta)}{\partial\bmbeta} = -2\bmX^\mT\bmy + 2\bmX^\mT\bmX\bmbeta$

<!-- ## Minimizing Sum of Squared Residuals -->
Set the derivative to zero:
\begin{align*}
0 &= -2\bmX^\mT\bmy + 2\bmX^\mT\bmX\hat\bmbeta\\
\bmX^\mT\bmX\hat\bmbeta &= \bmX^\mT\bmy \qquad \text{(LS `Normal' Equations})\\
\hat\bmbeta & = \left(\XtX\right)^{-1}\bmX^\mT\bmy
\end{align*}



### Relationship to SLR

What if $k=1$? Then $\bmX = \begin{bmatrix} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix}$ and  $\hat\bmbeta$ reduces to the $(\hat\beta_0, \hat\beta_1)$ from SLR:

\begin{align*}
\hat\bmbeta & = \left(\XtX\right)^{-1}\bmX^\mT\bmy\\
& = \left(\begin{bmatrix} 1 & \cdots & 1 \\ x_{1} & \cdots &  x_{n}\end{bmatrix}\begin{bmatrix} 1 & x_{1} \\ \vdots & \vdots \\ 1 & x_{n} \end{bmatrix}\right)^{-1} \begin{bmatrix} 1 & \cdots & 1 \\ x_{1} & \cdots &  x_{n}\end{bmatrix}\begin{bmatrix} y_{1} \\ \vdots \\ y_{n} \end{bmatrix}\\
&= \left(\begin{bmatrix} n & \displaystyle\sum_{i=1}^n {x_i} \\ \displaystyle\sum_{i=1}^n {x_i} & \displaystyle \sum_{i=1}^n {x_i}^2 \end{bmatrix}\right)^{-1} \begin{bmatrix} \displaystyle\sum_{i=1}^n y_i \\ \displaystyle\sum_{i=1}^n x_iy_i  \end{bmatrix}\\
\end{align*}



\begin{align*}
&=\frac{1}{\displaystyle n\sum_{i=1}^n {x_i}^2  - \left(\sum_{i=1}^n {x_i}\right)^2} \left(\begin{bmatrix} \displaystyle \sum_{i=1}^n {x_i}^2  &\displaystyle  -\sum_{i=1}^n {x_i} \\ \displaystyle -\sum_{i=1}^n {x_i} & n \end{bmatrix}\right) \begin{bmatrix} \sum_{i=1}^n y_i \\ \displaystyle \sum_{i=1}^n x_iy_i  \end{bmatrix}\\
&=\frac{1}{\displaystyle n\sum_{i=1}^n {x_i}^2  -
\left(\sum_{i=1}^n {x_i}\right)^2} \left(\begin{bmatrix} \displaystyle \sum_{i=1}^n {x_i}^2 \sum_{i=1}^n y_i  -\sum_{i=1}^n {x_i} \sum_{i=1} y_i  \\ \displaystyle -\sum_{i=1}^n {x_i}\sum_{i=1}^n y_i  +  n\sum_{i=1}^n x_iy_i \end{bmatrix}\right)\\
\Rightarrow \hat\beta_1 &= \frac{\displaystyle n\sum_{i=1}^n x_iy_i -\sum_{i=1}^n {x_i}\sum_{i=1}^n y_i }{\displaystyle n\sum_{i=1}^n {x_i}^2  - \left(\sum_{i=1}^n {x_i}\right)^2} = \frac{S_{xy}}{S_{xx}}
\end{align*}



## Why must X be full-rank?

What does it mean for $\bmX$ to be 'full-rank'?

* Conceptually: Each column of $\bmX$ is providing different information; nothing is duplicated.
    * E.g. including speed in miles per hour and kilometers per hour is redundant.
* Mathematically: The columns of $\bmX$ are linearly independent, so the dimension of the column space of $\bmX$ is equal to the number of columns in $\bmX$
    * For example, if $\bmx_3 = \bmx_1 + \bmx_2$, then the matrix $\bmX = \begin{bmatrix} \mathbf{1} & \bmx_1 & \bmx_2 & \bmx_3 \end{bmatrix}$ is not full rank (it has rank = 3) because the column $\bmx_3$ can be written as a linear combination of columns $\bmx_1$ and $\bmx_2$.

If $\bmX$ is less than full rank, then $(\XtX)^{-1}$ does not exist.


Practical implications:

* Each variable in the model should provide new information (i.e. no duplicates)
* R will usually drop variables it identifies as redundant.






## Fitting MLR in \texttt{R}
To fit a MLR model in \texttt{R}, use the \texttt{lm()} command. In the formula, separate predictor variables by \texttt{+}: `y ~ x1 + x2 + x3`.
<!-- (the intercept is automatically included). -->

```{r eval=FALSE, echo=TRUE, size="footnotesize"}
nfl_lm <- lm(yards_gained ~ yardline_100 + ydstogo + play_type,
             data=nfl_den)
```


```{r eval=FALSE, echo=TRUE, size="footnotesize"}
library(broom)
tidy(nfl_lm)
```



```{r eval=FALSE, echo=TRUE, size="footnotesize", output.lines=1:9}
summary(nfl_lm)
```


```{r eval=FALSE, echo=TRUE, size="footnotesize", output.lines=-1:-9}
summary(nfl_lm)
```


### Fitting MLR in \texttt{R} -- Duplicated variable


```{r eval=FALSE, echo=TRUE, size="footnotesize", output.lines=10:16}
# Create fttogo, which duplicates ydstogo
nfl_den$fttogo <- 3*nfl_den$ydstogo
# Fit model with fttogo
nfl_lm2 <- lm(yards_gained ~ yardline_100 + ydstogo + 
                play_type + fttogo, data=nfl_den)
summary(nfl_lm2)
```

## Properties of OLS Estimators

<!-- ## Properties of OLS Estimators -->
<!-- We want to know the mean and variance of $\hat\bmbeta$.  Why? -->

<!-- * Mean: We want that, on average, $\hat\bmbeta$ is providing the correct value.  -->
<!--     * We want $\hat\bmbeta$ to be unbiased, so $\E[\hat\bmbeta] = \bmbeta$. -->
<!-- * Variance: WE will use $We want to know how variable (across repeated datasets) $\hat\bmbeta$ is. Suppose we obtain $\hat\bmbeta = 1$. Is this meaningfully different from $0$? If $\Var(\hat\bmbeta) = 100$, then no. If $\Var(\hat\bmbeta) = 0.0001$, then yes. -->

### Mean of $\hat\bmbeta$

Like in SLR, the OLS estimator $\hat\bmbeta$ is unbiased:
\begin{align*}
\E[\hat\bmbeta] &= \E\left[(\XtX)^{-1}\bmX^\mT\bmy\right]\\
& = (\XtX)^{-1}\bmX^\mT\E\left[\bmy\right]\\
&= (\XtX)^{-1}\bmX^\mT\left(\bmX\bmbeta + \E[\boldsymbol\epsilon]\right)\\
&= (\XtX)^{-1}\bmX^\mT\bmX\bmbeta\\
&= \bmbeta
\end{align*}

### Variance of $\hat\bmbeta$

What about the variance of $\hat\bmbeta$?  

Recall that if $\mathbf{A}$ is a $q \times n$ matrix and $\bmy$ is an $n$-vector,  $\Var\left[\mathbf{A}\bmy\right] = \mathbf{A}\Var\left[\bmy\right]\mathbf{A}^\mT$ (a $q \times q$ matrix).

\begin{align*}
\Var(\hat\bmbeta) &= \Var\left((\XtX)^{-1}\bmX^\mT\bmy\right)\\
&= (\XtX)^{-1}\bmX^\mT\Var(\bmy)\left((\XtX)^{-1}\bmX^\mT\right)^\mT\\
&= (\XtX)^{-1}\bmX^\mT\sigma^2\bmI\bmX(\XtX)^{-1}\\
&= \sigma^2(\XtX)^{-1}\bmX^\mT\bmX(\XtX)^{-1}\\
&= \sigma^2(\XtX)^{-1}
\end{align*}


$\Var(\hat\bmbeta) = \sigma^2(\XtX)^{-1}$ is a matrix. 

* The diagonal elements of $\Var(\hat\bmbeta)$ are the variances of each $\beta_j$.
* The off-diagonal elements provide the *co*variances of the elements of $\beta_j$. 
* Does this form look familiar to $\Var(\hat\beta_1)$ from SLR?

<!-- These will be important when we look at testing and confidence intervals for combinations of variables. -->



## Fitted values and residuals

### Fitted values for MLR

Fitted values for the MLR model are $\hat y_i = \hat\beta_0 + \hat\beta_1x_{i1} + \dots \hat\beta_kx_{ik}$.

In matrix form, this is:
\begin{align*}
\hat\bmy &= \bmX\hat\bmbeta\\
&= \bmX(\XtX)^{-1}\bmX^\mT\bmy\\
&= \left[\bmX(\XtX)^{-1}\bmX^\mT\right]\bmy\\
&= \bmH \bmy
\end{align*}


### Hat Matrix $\bmH$


$\bmH = \bmX(\XtX)^{-1}\bmX^\mT$ is called the `hat' matrix

* Conceptual Interpretation of $\bmH$
    * Uses predictor variables ($x$'s) to transform observations ($y$'s) to fitted values ($\hat y$'s).
    * Elements of $\bmH$ relate to *leverage* and *influence*, which we will discuss in later lecture  (Ch 4 in book).
* Mathematical Properties of $\bmH$
    * Symmetric ($\bmH^\mT=\bmH$)
    * A projection matrix into the column space of $\bmX$
    * Idempotent ($\bmH\bmH = \bmH$)
    * $\bmH(\bmI - \bmH) = (\bmI - \bmH)\bmH = \mathbf{0}$ (this follows from $\bmH$ being idempotent)


### Residuals for MLR

Residuals are defined like SLR: $e_i = y_i - \hat y_i$. 

Using the matrix form of the model, we can write this:
\begin{align*}
\mathbf{e} &= \bmy - \bmX\hat\bmbeta\\
&= \bmy - \bmH\bmy \\
&= (\mathbf{I} - \bmH)\bmy
\end{align*}


## Estimating $\sigma^2$
Recall that $\sigma^2 = \text{Var}(\epsilon_i)$.  
In SLR, this was the 'error' above and below the line given by the model. In MLR, this is the 'error' above and below the *hyperplane*  given by the model.

Like SLR, we estimate $\sigma^2$ using $MS_{Res}$. First compute $SS_{Res}$:
\begin{align*}
SS_{Res} & = \sum_{i=1}^n (y_i - \hat y_i)^2\\
&= \sum_{i=1}^n e_i^2\\
&= \mathbf{e}^\mT\mathbf{e}\\
&= [(\mathbf{I} - \bmH)\bmy]^\mT(\mathbf{I} - \bmH)\bmy\\
&= \bmy^\mT(\mathbf{I} - \bmH)^\mT(\mathbf{I} - \bmH)\bmy\\
&= \bmy^\mT(\mathbf{I} - \bmH)\bmy
\end{align*}


How many degrees of freedom are there?  
$n-p$, since there are $p$ parameters estimated ($\hat\beta_0, \hat\beta_1, \dots, \hat\beta_k$).


<!-- *Aside for those interested: the rank of $\bmI - \bmH$ is $n-p$, which is how this degrees of freedom is derived.* -->

The residual mean square is then:
$$ MS_{Res} = \frac{SS_{Res}}{n-p}$$


## Estimating $\Var(\hat\bmbeta)$
How do we estimate $\Var(\hat\bmbeta)$?  

* $(\XtX)^{-1}$ is a function of $\bmX$, which is fixed and known
* Need an estimate of $\sigma^2$: use $MS_{Res}$.
$$\widehat{\Var}(\hat\bmbeta) = MS_{Res}(\XtX)^{-1}$$
The square-root of the diagonal elements of $\widehat{\Var}(\hat\bmbeta)$ are the standard errors of $\hat\beta_j$.

### Estimating $\Var(\hat\bmbeta)$ in R
The \texttt{vcov} function will compute $\widehat{\Var}(\hat\bmbeta)$:

```{r eval=FALSE, echo=TRUE, size="footnotesize"}
vcov(nfl_lm)
```


You can compute standard errors from $\widehat{\Var}(\hat\bmbeta)$:

```{r eval=FALSE, echo=TRUE, size="small"}
diag(vcov(nfl_lm))
sqrt(diag(vcov(nfl_lm)))
tidy(nfl_lm)
```